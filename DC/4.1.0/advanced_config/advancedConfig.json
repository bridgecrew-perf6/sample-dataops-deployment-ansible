"advancedConfiguration": "[ { "fileName" : "sdc.properties",\n  "fileContent" : "#\\n# Copyright (c) 2021 StreamSets Inc.\\n#\\n\\n# HTTP configuration\\n\\n# The base URL of Data Collector, used to create email alert messages.\\n# If not set http://<hostname>:<http.port> is used\\n# <hostname> is either taken from http.bindHost or resolved using\\n# 'hostname -f' if not configured.\\n#sdc.base.http.url=http://<hostname>:<port>\\n\\n# Hostname or IP address that Data Collector will bind to.\\n# Default is 0.0.0.0 that will bind to all interfaces.\\n#http.bindHost=0.0.0.0\\n\\n# Maximum number of HTTP servicing threads.\\n#http.maxThreads=200\\n\\n# The port Data Collector uses as an HTTP endpoint.\\n# If different from -1, the Data Collector will run on this port\\n# If 0, the Data Collector will pick up a random port\\n# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint\\n# will redirect to the HTTPS endpoint.\\nhttp.port=18630\\n\\n# HTTPS configuration\\n\\n# TThe port Data Collector uses as an HTTPS endpoint.\\n# If different from -1, the Data Collector will run over SSL on this port\\n# If 0, the Data Collector will use a random port\\nhttps.port=-1\\n\\n# Enables HTTP/2 support for the Data Collector UI/REST API. If you are using any clients\\n# that do not support ALPN for protocol negotiation, leave this option disabled.\\nhttp2.enable=false\\n\\n# Reverse Proxy / Load Balancer configuration\\n\\n# Data Collector will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port\\n# headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.\\n# Set to true when hosting Data Collector behind a reverse proxy / load balancer.\\nhttp.enable.forwarded.requests=false\\n\\n# Java keystore file, in the Data Collector 'etc/' configuration directory\\nhttps.keystore.path=keystore.jks\\n\\n# Password for the keystore file,\\n# By default, the password is loaded from 'keystore-password.txt'\\n# in the Data Collector 'etc/' configuration directory\\nhttps.keystore.password=${file(\\"keystore-password.txt\\")}\\n\\n# Path to keystore file on the worker node. This should always be an absolute location\\nhttps.cluster.keystore.path=/opt/security/jks/sdc-keystore.jks\\n\\n# Password for keystore file on the worker node\\nhttps.cluster.keystore.password=${file(\\"/opt/security/jks/keystore-password.txt\\")}\\n\\n# Truststore configs\\n# By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore\\n\\n# Java truststore file on the gateway Data Collector which stores certificates to trust identity of workers\\n#https.truststore.path=\\n\\n# Password for truststore file\\n#https.truststore.password=\\n\\n# Path to truststore file on worker node. This should always be an absolute location\\n#https.cluster.truststore.path=/opt/security/jks/sdc-truststore.jks\\n\\n# Password for truststore file on worker\\n#https.cluster.truststore.password=${file(\\"/opt/security/jks/truststore-password.txt\\")}\\n\\n# HTTP Session Timeout\\n# Max period of inactivity, after which the HTTP session is invalidated, in seconds.\\n# Default value is 86400 seconds (24 hours)\\n# value -1 means no timeout\\nhttp.session.max.inactive.interval=86400\\n\\n# The authentication for the HTTP endpoint of Data Collector\\n# Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'\\n#\\nhttp.authentication=form\\n\\n# Authentication Login Module\\n# Valid values are: 'file' and 'ldap'\\n# For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,\\n#   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).\\n# For 'ldap', the authentication and role information is read from a LDAP server\\n# and LDAP connection information is read from etc/ldap-login.conf.\\nhttp.authentication.login.module=file\\n\\n# The realm used for authentication\\n# A file with the realm name and '.properties' extension must exist in the Data Collector configuration directory\\n# If this property is not set, the realm name is '<http.authentication>-realm'\\n#http.digest.realm=local-realm\\n\\n# Check the permissions of the realm file should be owner only\\nhttp.realm.file.permission.check=true\\n\\n# LDAP group to Data Collector role mapping\\n# the mapping is specified as the following pattern:\\n#    <ldap-group>:<sdc-role>(,<sdc-role>)*(;<ldap-group>:<sdc-role>(,<sdc-role>)*)*\\n# e.g. Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;\\nhttp.authentication.ldap.role.mapping=\\n\\n# LDAP login module name as present in the JAAS config file.\\n# If no value is specified, the login module name is assumed to be \\"ldap\\"\\nldap.login.module.name=ldap\\n\\n# HTTP access control (CORS)\\nhttp.access.control.allow.origin=*\\nhttp.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call\\nhttp.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET\\nhttp.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD\\n\\n# Runs the Data Collector within a Kerberos session which is propagated to all stages.\\n# This is useful for stages that require Kerberos authentication with the services they interact with\\nkerberos.client.enabled=false\\n\\n# The Kerberos principal to use for the Kerberos session.\\n# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',\\n# the hostname will be replaced with the actual complete hostname of Data Collector as advertised by the\\n# unix command 'hostname -f'\\nkerberos.client.principal=sdc/_HOST@EXAMPLE.COM\\n\\n# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be\\n# looked for in the Data Collector configuration directory\\nkerberos.client.keytab=sdc.keytab\\n\\n# Maximal batch size for preview\\npreview.maxBatchSize=1000\\n# Maximal number of batches for preview\\npreview.maxBatches=10\\n# Maximal batch size for pipeline run\\nproduction.maxBatchSize=50000\\n\\n#Specifies the buffer size for Overrun parsers - including JSON, XML and CSV.\\n#This parameter is specified in bytes, and must be greater than\\n#1048576 bytes (which is the default size).\\n#parser.limit=5335040\\n\\n#This option determines the number of error records, per stage, that will be retained in memory when the pipeline is\\n#running. If set to zero, error records will not be retained in memory.\\n#If the specified limit is reached the oldest records will be discarded to make room for the newest one.\\nproduction.maxErrorRecordsPerStage=100\\n\\n#This option determines the number of pipeline errors that will be retained in memory when the pipeline is\\n#running. If set to zero, pipeline errors will not be retained in memory.\\n#If the specified limit is reached the oldest error will be discarded to make room for the newest one.\\nproduction.maxPipelineErrors=100\\n\\n# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint\\nmax.logtail.concurrent.requests=5\\n\\n# Max number of concurrent WebSocket calls allowed\\nmax.webSockets.concurrent.requests=15\\n\\n# Pipeline Sharing / ACLs\\npipeline.access.control.enabled=false\\n\\n# Customize header title for the Data Collector UI\\n# You can pass any HTML tags here\\n# Example:\\n#   For Text  -  <span class=\\"navbar-brand\\">New Brand Name</span>\\n#   For Image -  <img src=\\"assets/add.png\\">\\nui.header.title=\\n\\nui.local.help.base.url=/docs\\nui.hosted.help.base.url=https://docs.streamsets.com/portal/#datacollector/latest/help\\n# ui.registration.url=https://registration.streamsets.com/register\\n# ui.account.registration.url=\\n\\nui.refresh.interval.ms=2000\\nui.jvmMetrics.refresh.interval.ms=4000\\n\\n# If set to true, the Data Collector UI will use WebSocket to fetch pipeline status/metrics/alerts. Otherwise, the UI\\n# will poll every few seconds to get the pipeline status/metrics/alerts.\\nui.enable.webSocket=true\\n\\n# Number of changes supported by undo/redo functionality.\\n# UI archives pipeline configuration/rules in browser memory to support undo/redo functionality.\\nui.undo.limit=10\\n\\n# Default mode for stage configuration view unless specifically selected by\\n# the user on the canvas for the pipeline stage.\\n# Set to ADVANCED to show advanced configurations by default\\n# ui.default.configuration.view=BASIC\\n\\n# SMTP configuration to send alert emails\\n# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'\\nmail.transport.protocol=smtp\\nmail.smtp.host=localhost\\nmail.smtp.port=25\\nmail.smtp.auth=false\\nmail.smtp.starttls.enable=false\\nmail.smtps.host=localhost\\nmail.smtps.port=465\\nmail.smtps.auth=false\\n# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,\\n# ${file(\\"email-password.txt\\")} will load the value from the 'email-password.txt' file in the config directory (where this file is)\\nxmail.username=foo\\nxmail.password=${file(\\"email-password.txt\\")}\\n# FROM email address to use for the messages\\nxmail.from.address=sdc@localhost\\n\\n#Indicates the location where runtime configuration properties can be found.\\n#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with\\n#'runtime.conf_'.\\n#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration\\n#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.\\nruntime.conf.location=embedded\\n\\n# Java Security properties\\n#\\n# Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security\\n# as part of Data Collector bootstrap process. This will change JVM configuration and should not be used when embedding and running\\n# multiple Data Collector instances inside the same JVM.\\n#\\n# We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.\\njava.security.networkaddress.cache.ttl=0\\n\\n# Security Manager\\n#\\n# By default, when Security Manager is enabled, Data Collector will use Java security manager that only follows the specified policy.\\n\\n# Enable the Data Collector Security Manager to prevent access to Data Collector internal directories to all stages (e.g. data dir,\\n# ...). Please note that there are certain JVM bugs that this manager might hit, especially on some older JVM versions.\\n#security_manager.sdc_manager.enable=true\\n\\n# When Security Manager is enabled Data Collector will by default prohibit access to its internal directories regardless of what\\n# the security policy specifies. The following properties allow specific access to given files inside protected directories.\\n# General exceptions - use with caution, all stage libraries will be able to access those files.\\n# * Access to ldap-login.conf is sadly required by the Hadoop UserGroupInfo class\\nsecurity_manager.sdc_dirs.exceptions=$SDC_CONF/ldap-login.conf\\n\\n# Exceptions for specific stage libraries\\n# * Our documentation recommends default name for credential store inside ETC directory\\nsecurity_manager.sdc_dirs.exceptions.lib.streamsets-datacollector-jks-credentialstore-lib=$SDC_CONF/jks-credentialStore.pkcs12\\n\\n\\n# Stage-specific configurations\\n#\\n# The following config properties are for particular stages, please refer to their documentation for further details.\\n#\\n# Hadoop components\\n# Uncomment to enforce Hadoop components in Data Collector to always impersonate current user rather then use\\n# the impersonation configuration option. Current user is a user who either started the pipeline or run preview.\\n#stage.conf_hadoop.always.impersonate.current.user=true\\n# Uncomment to enforce impersonated user name to be lower cased.\\n#stage.conf_hadoop.always.lowercase.user=true\\n#\\n\\n# Impersonate current user to connect to Hive\\nstage.conf_com.streamsets.pipeline.stage.hive.impersonate.current.user=false\\n\\n# JDBC components\\n# Drivers that should always be auto-loaded even if they are not JDBC 4 compliant or fails to load (comma separated list)\\n#stage.conf_com.streamsets.pipeline.stage.jdbc.drivers.load=mysql.jdbc.Driver\\n\\n# Kafka stages\\n# Controls where Kerberos authentication keytabs entered in stage properties are stored\\n# stage.conf_kafka.keytab.location=/tmp/sdc\\n\\n# Use new version of addRecordsToQueue() in Oracle CDC origin\\nstage.conf_com.streamsets.pipeline.stage.origin.jdbc.cdc.oracle.addrecordstoqueue=true\\n\\n# Shell executor\\n# Controls impersonation mode\\n#stage.conf_com.streamsets.pipeline.stage.executor.shell.impersonation_mode=CURRENT_USER\\n# Relative or absolute path to shell that should be used to execute the shell script\\n#stage.conf_com.streamsets.pipeline.stage.executor.shell.shell=sh\\n# Relative or absolute path to sudo command\\n#stage.conf_com.streamsets.pipeline.stage.executor.shell.sudo=sudo\\n\\n# Used by SCH Orchestration stage to allow API User Credentials or not\\nstage.conf_com.streamsets.pipeline.stage.orchestration.schApiUserCredentials.enabled=true\\n\\n# Antenna Doctor\\n\\n# Antenna Doctor is a rule-based engine designed to help end-user self-diagnose most common issues and suggest\\n# potential fixes and workarounds.\\n\\n# Uncomment to disable Antenna Doctor completely\\n#antennadoctor.enable=false\\n\\n# Uncomment to disable periodical updates of the knowledge base from internet\\n#antennadoctor.update.enable=false\\n\\n#Observer related\\n\\n#The size of the queueName where the pipeline queues up data rule evaluation requests.\\n#Each request is for a stream and contains sampled records for all rules that apply to that lane.\\nobserver.queue.size=100\\n\\n#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is\\n#once cache per data rule\\nobserver.sampled.records.cache.size=100\\n\\n#The time to wait before dropping a data rule evaluation request if the observer queueName is full.\\nobserver.queue.offer.max.wait.time.ms=1000\\n\\n\\n#Maximum number of private classloaders to allow in Data Collector.\\n#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders\\nmax.stage.private.classloaders=50\\n\\n# Pipeline runner pool\\n# Default value is sufficient to run 22 pipelines. One pipeline requires 5 threads and pipelines share\\n# threads using thread pool. Approximate runner thread pool size = (Number of Running Pipelines) * 2.2.\\n# Increasing this value will not increase parallelisation of individual pipelines.\\nrunner.thread.pool.size=50\\n\\n# Uncomment to disable starting all previously running pipelines upon Data Collector start up\\n#runner.boot.pipeline.restart=false\\n\\n# Maximal number of runners (multithreaded pipelines)\\n#\\n# Maximal number of source-less pipeline instances (=runners) that are allowed for a single multi-threaded\\n# pipeline. The default is 50.\\npipeline.max.runners.count=50\\n\\n# Uncomment to specify a custom location for Package Manager repositories.\\n# Enter a url or comma-separated list of urls.\\n# Official Data Collector releases use the following repositories by default:\\n# http://archives.streamsets.com/datacollector/<version>/tarball/,\\n# http://archives.streamsets.com/datacollector/<version>/tarball/enterprise/ and\\n# http://archives.streamsets.com/datacollector/<version>/legacy/\\n# Data Collector source code builds (master branch) use the following repositories by default:\\n# http://nightly.streamsets.com/datacollector/latest/tarball/,\\n# http://nightly.streamsets.com/datacollector/latest/tarball/enterprise/ and\\n# http://nightly.streamsets.com/datacollector/latest/legacy/ and\\n#package.manager.repository.links=\\n\\n# Support bundles\\n#\\n# Uncomment if you need to disable the facility for automatic support bundle upload.\\n#bundle.upload.enabled=false\\n#\\n# Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle\\n# can be time consuming task (depending on size and internet speed) and pipelines can appear \\"frozen\\" during\\n# the upload especially when many pipelines are failing at the same time.\\n#bundle.upload.on_error=true\\n\\n# Library aliases mapping to keep backward compatibility on pipelines when library names change\\n# The current aliasing mapping is to handle 1.0.0beta2 to 1.0.0 library names changes\\n#\\n# IMPORTANT: Under normal circumstances all these properties should not be changed\\n#\\nlibrary.alias.streamsets-datacollector-apache-kafka_0_8_1_1-lib=streamsets-datacollector-apache-kafka_0_8_1-lib\\nlibrary.alias.streamsets-datacollector-apache-kafka_0_8_2_0-lib=streamsets-datacollector-apache-kafka_0_8_2-lib\\nlibrary.alias.streamsets-datacollector-apache-kafka_0_8_2_1-lib=streamsets-datacollector-apache-kafka_0_8_2-lib\\nlibrary.alias.streamsets-datacollector-cassandra_2_1_5-lib=streamsets-datacollector-cassandra_2-lib\\nlibrary.alias.streamsets-datacollector-cdh5_2_1-lib=streamsets-datacollector-cdh_5_2-lib\\nlibrary.alias.streamsets-datacollector-cdh5_2_3-lib=streamsets-datacollector-cdh_5_2-lib\\nlibrary.alias.streamsets-datacollector-cdh5_2_4-lib=streamsets-datacollector-cdh_5_2-lib\\nlibrary.alias.streamsets-datacollector-cdh5_3_0-lib=streamsets-datacollector-cdh_5_3-lib\\nlibrary.alias.streamsets-datacollector-cdh5_3_1-lib=streamsets-datacollector-cdh_5_3-lib\\nlibrary.alias.streamsets-datacollector-cdh5_3_2-lib=streamsets-datacollector-cdh_5_3-lib\\nlibrary.alias.streamsets-datacollector-cdh5_4_0-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib\\nlibrary.alias.streamsets-datacollector-cdh5_4_0-lib=streamsets-datacollector-cdh_5_4-lib\\nlibrary.alias.streamsets-datacollector-cdh5_4_1-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib\\nlibrary.alias.streamsets-datacollector-cdh5_4_1-lib=streamsets-datacollector-cdh_5_4-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib\\nlibrary.alias.streamsets-datacollector-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_kafka_1_2-lib\\nlibrary.alias.streamsets-datacollector-elasticsearch_1_4_4-lib=streamsets-datacollector-elasticsearch_1_4-lib\\nlibrary.alias.streamsets-datacollector-elasticsearch_1_5_0-lib=streamsets-datacollector-elasticsearch_1_5-lib\\nlibrary.alias.streamsets-datacollector-hdp_2_2_0-lib=streamsets-datacollector-hdp_2_2-lib\\nlibrary.alias.streamsets-datacollector-jython_2_7_0-lib=streamsets-datacollector-jython_2_7-lib\\nlibrary.alias.streamsets-datacollector-mongodb_3_0_2-lib=streamsets-datacollector-mongodb_3-lib\\nlibrary.alias.streamsets-datacollector-cassandra_2-lib=streamsets-datacollector-cassandra_3-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_9-cluster-cdh_kafka_2_0-lib=streamsets-datacollector-cdh-spark_2_1-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_10-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_11-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_12-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_13-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\\nlibrary.alias.streamsets-datacollector-cdh_5_14-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\\n\\n\\n# Stage aliases for mapping to keep backward compatibility on pipelines when stages move libraries\\n# The current alias mapping is to handle moving the jdbc stages to their own library\\n#\\n# IMPORTANT: Under normal circumstances all these properties should not be changed\\n#\\nstage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget\\nstage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource\\nstage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_omniture_OmnitureDSource=streamsets-datacollector-omniture-lib,com_streamsets_pipeline_stage_origin_omniture_OmnitureDSource\\nstage.alias.streamsets-datacollector-cdh_5_7-cluster-cdh_kafka_2_0-lib,com_streamsets_pipeline_stage_destination_kafka_KafkaDTarget=streamsets-datacollector-cdh_kafka_2_0-lib,com_streamsets_pipeline_stage_destination_kafka_KafkaDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_6-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_7-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_1-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_2-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_3-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_5_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_6-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_1_7-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_1-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_2-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_3-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_2_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-elasticsearch_5_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\\nstage.alias.streamsets-datacollector-cdh_5_4-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\\nstage.alias.streamsets-datacollector-cdh_5_5-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\\nstage.alias.streamsets-datacollector-cdh_5_7-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\\nstage.alias.streamsets-datacollector-cdh_5_8-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\\nstage.alias.streamsets-datacollector-cdh_5_9-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\\nstage.alias.streamsets-datacollector-cdh_5_10-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_FirehoseDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_FirehoseDTarget\\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_StatsKinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_StatsKinesisDTarget\\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_KinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_KinesisDTarget\\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_ToErrorKinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_ToErrorKinesisDTarget\\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_origin_kinesis_KinesisDSource=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_origin_kinesis_KinesisDSource\\nstage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor\\nstage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget\\nstage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget\\nstage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor\\nstage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget\\nstage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget\\nstage.alias.streamsets-datacollector-couchbase_5-lib,com_streamsets_pipeline_stage_destination_couchbase_CouchbaseConnectorDTarget=streamsets-datacollector-couchbase_5-lib,com_streamsets_pipeline_stage_destination_couchbase_CouchbaseDTarget\\n\\n\\n# System and user stage libraries whitelists and blacklists\\n#\\n# If commented out all stagelibraries directories are used.\\n#\\n# Given 'system' or 'user', only whitelist or blacklist can be set, if both are set the Data Collector will fail to start\\n#\\n# Specify stage library directories separated by commas\\n#\\n# The MapR stage libraries are disabled as they require manual installation step. Use the setup-mapr script to enable\\n# the desired MapR stage library.\\n#\\n# It's important to keep the blacklist and whitelist properties on a single line, otherwise CSD's control.sh script and\\n# setup-mapr script will not work properly.\\n#\\n#system.stagelibs.whitelist=\\nsystem.stagelibs.blacklist=streamsets-datacollector-mapr_6_0-lib,streamsets-datacollector-mapr_6_0-mep4-lib,streamsets-datacollector-mapr_6_0-mep5-lib,streamsets-datacollector-mapr_6_1-lib,streamsets-datacollector-mapr_6_1-mep6-lib\\n#\\n#user.stagelibs.whitelist=\\n#user.stagelibs.blacklist=\\n\\n# Stage Classpath Validation\\n#\\n# Uncomment to disable best effort validation of each stage library classpath to detect known issues with\\n# colliding dependencies (such as conflicting versions of the same dependency, ...). Result of the validation\\n# is by default only printed to log.\\n#stagelibs.classpath.validation.enable=false\\n#\\n# By default the validation result is only logged. Uncomment to prevent Data Collector to start if classpath of any\\n# stage library is not considered valid.\\n#stagelibs.classpath.validation.terminate=true\\n\\n# Health Inspector Configuration\\n#\\n# Configuration options specific to alter behavior of health inspector.\\n#\\n#health_inspector.network.host=www.streamsets.com\\n\\n#\\n# Additional configuration files to include in to the configuration.\\n# Value of this property is the name of the configuration file separated by commas.\\n#\\nconfig.includes=vault.properties,credential-stores.properties\\n\\n#\\n# Record sampling configurations indicate the size of the subset (sample set) that must be chosen from a population (of records).\\n# Default configuration values indicate the sampler to select 1 out of 10000 records\\n#\\n# For better performance simplify the fraction ( sdc.record.sampling.sample.size / sdc.record.sampling.population.size )\\n# i.e., specify ( 1 / 40 ) instead of ( 250 / 10000 ).\\nsdc.record.sampling.sample.size=1\\nsdc.record.sampling.population.size=10000\\n\\n#\\n# Pipeline state are cached for faster access.\\n# Specifies the maximum number of pipeline state entries the cache may contain.\\nstore.pipeline.state.cache.maximum.size=100\\n\\n# Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration\\n# has elapsed after the entry's creation, the most recent replacement of its value, or its last access.\\n# In minutes\\nstore.pipeline.state.cache.expire.after.access=10\\n\\n#\\n# Copyright (c) 2021 StreamSets Inc.\\n#\\n\\n#\\n# Control Hub Enabled\\n# If true HTTP Authentication for Data Collector will be configured to use Control Hub SSO Authentication.\\n#\\ndpm.enabled=false\\n\\n#\\n# Base URL of the Remote Service\\n# In a real deployment the security service must be encrypted (HTTPS)\\n#\\ndpm.base.url=@dpm-url.txt@\\n\\n#\\n# Registration attempts.\\n# There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.\\n#\\ndpm.registration.retry.attempts=5\\n\\n#\\n# Frequency of validation of user and app authentication tokens.\\n# As part of this validation all information about the principal is refreshed.\\n#\\ndpm.security.validationTokenFrequency.secs=60\\n\\n#\\n# Application Token\\n#\\ndpm.appAuthToken=@application-token.txt@\\n\\n#\\n# Labels for this Data Collector to report the Control Hub\\n#\\ndpm.remote.control.job.labels=all\\n\\n#\\n# Data Collector Ping Frequency to Control Hub (in milliseconds)\\n#\\ndpm.remote.control.ping.frequency=5000\\n\\n#\\n# App to send remote control events\\n#\\ndpm.remote.control.events.recipient=jobrunner-app\\n\\n#\\n# Apps to send Data Collector Process metrics (CPU Load and Heap Memory Usage)\\n#\\ndpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app\\n\\n#\\n# Frequency to send pipeline status events (all remote pipelines and local running pipelines) and\\n# Data Collector process metrics like CPU load and heap memory usage\\n#\\ndpm.remote.control.status.events.interval = 60000\\n\\n\\ndpm.remote.deployment.id=\\n\\ndpm.remote.csp.deployment.id=\\n\\n#\\n# Indicates if the redirection to Control Hub SSO is done using HTML META refresh.\\n# This is useful for environment that rewrite redirect headers.\\n#\\nhttp.meta.redirect.to.sso=false\\n\\n\\n# Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting\\n# only takes effect when sdc is control hub enabled and stage impersonation is set to true\\n#\\n#dpm.alias.name.enabled=true\\n"\n} ]"
