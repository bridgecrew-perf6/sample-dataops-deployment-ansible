\"#\\n# Copyright (c) 2021 StreamSets Inc.\\n#\\n\\n# HTTP configuration\\n\\n# The base URL of Transformer, used to create email alert messages.\\n# If not set http://<hostname>:<http.port> is used.\\n# <hostname> is either taken from http.bindHost or resolved using\\n# 'hostname -f' if not configured.\\n#transformer.base.http.url=http://<hostname>:<port>\\n\\n# Hostname or IP address that Transformer will bind to.\\n# Default is 0.0.0.0 that will bind to all interfaces.\\n#http.bindHost=0.0.0.0\\n\\n# Maximum number of HTTP servicing threads.\\n#http.maxThreads=200\\n\\n# The port Transformer runs as the Transformer HTTP endpoint.\\n# If different that -1, Transformer will run on this port.\\n# If 0, Transformer will pick up a random port.\\n# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint\\n# will redirect to the HTTPS endpoint.\\nhttp.port=19630\\n\\n# HTTPS configuration\\n\\n# The port Transformer runs as the Transformer HTTPS endpoint.\\n# If different that -1, Transformer will run over SSL on this port.\\n# If 0, the Transformer will pick up a random port.\\nhttps.port=-1\\n\\n# Enables HTTP/2 support for the Transformer UI/REST API. If you are using any clients\\n# that do not support ALPN for protocol negotiation, leave this option disabled.\\nhttp2.enable=false\\n\\n# Reverse Proxy / Load Balancer configuration\\n\\n# TRANSFORMER will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port\\n# headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.\\n# Set to true when hosting TRANSFORMER behind a reverse proxy / load balancer.\\nhttp.enable.forwarded.requests=false\\n\\n# Java keystore file, in the TRANSFORMER 'etc/' configuration directory\\nhttps.keystore.path=keystore.jks\\n\\n# Password for the keystore file,\\n# By default, the password is loaded from the 'keystore-password.txt'\\n# from the TRANSFORMER 'etc/' configuration directory\\nhttps.keystore.password=${file(\\\"keystore-password.txt\\\")}\\n\\n# Truststore configs\\n# By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore\\n\\n# Java truststore file on Spark driver transformer which stores certificates to trust identity of transformer launcher\\n# in the TRANSFORMER 'etc/' configuration directory.\\n#https.truststore.path=truststore.jks\\n\\n# Password for truststore file\\n# from the TRANSFORMER 'etc/' configuration directory\\n#https.truststore.password=${file(\\\"truststore-password.txt\\\")}\\n\\n# HTTP Session Timeout\\n# Max period of inactivity, after which the HTTP session is invalidated, in seconds.\\n# Default value is 86400 seconds (24 hours)\\n# value -1 means no timeout\\nhttp.session.max.inactive.interval=86400\\n\\n# The authentication for the HTTP endpoint of Transformer.\\n# Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'\\n#\\nhttp.authentication=form\\n\\n# Authentication Login Module\\n# Valid values are: 'file' and 'ldap'\\n# For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,\\n#   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).\\n# For 'ldap', the authentication and role information is read from a LDAP Server\\n#   and LDAP connection information is read from etc/ldap-login.conf.\\nhttp.authentication.login.module=file\\n\\n# The realm used for authentication\\n# A file with the realm name and '.properties' extension must exist in the Transformer configuration directory\\n# If this property is not set, the realm name is '<http.authentication>-realm'\\n#http.digest.realm=local-realm\\n\\n# Check the permissions of the realm file should be owner only\\nhttp.realm.file.permission.check=true\\n\\n# LDAP group to Transformer role mapping\\n# the mapping is specified as the following pattern:\\n#    <ldap-group>:<transformer-role>(,<transformer-role>)*(;<ldap-group>:<transformer-role>(,<transformer-role>)*)*\\n# e.g.\\n#    Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;\\nhttp.authentication.ldap.role.mapping=\\n\\n# LDAP login module name as present in the JAAS config file.\\n# If no value is specified, the login module name is assumed to be \\\"ldap\\\"\\nldap.login.module.name=ldap\\n\\n# HTTP access control (CORS)\\nhttp.access.control.allow.origin=*\\nhttp.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call\\nhttp.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET\\nhttp.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD\\n\\n# Application callback inactivity timeout in milliseconds\\n# Max period of Transformer Spark application callback request inactivity, before stopping the pipeline with a run error.\\n# The default value is 30000 milliseconds (30 seconds)\\ntransformer.driver.max.inactive.interval=30000\\n\\n# A maximum number of times to retry an unsuccessful pipeline start on Databricks Cluster.\\ntransformer.databricks.run.max.retries=2\\n\\n# Minimal interval in milliseconds between the start of the failed run, and the subsequent retry run on the Databricks cluster.\\ntransformer.databricks.run.retry.interval=10000\\n\\n# Always resolve Kerberos properties\\n# This option allows Transformer pipelines to reference the keytab and principal declared in this properties file\\n# If you have a valid kerberos.client.principal and kerberos.client.keytab set below, and want to make them\\n# available to pipelines within this Transformer instance, then comment out the following line\\n#kerberos.client.alwaysResolveProperties=true\\n\\n# The kerberos principal to use for the Kerberos session.\\n# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',\\n# the hostname will be replaced with the actual complete hostname of Transformer as advertised by the\\n# unix command 'hostname -f'\\nkerberos.client.principal=transformer/_HOST@EXAMPLE.COM\\n\\n# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be\\n# looked under the Transformer configuration directory\\nkerberos.client.keytab=transformer.keytab\\n\\n# Uncomment this line to disallow users to specify a keytab to use with pipelines.\\n# If this option is set to false, then the --proxy-user option will be set to spark-submit and the\\n# Transformer process will assume that a ticket cache exists already (i.e. is managed by a wrapper script\\n# such as k5start) and is therefore available for the spark-submit child process\\n#kerberos.pipeline.keytab.allowed=false\\n\\npreview.maxBatchSize=1000\\npreview.maxBatches=10\\n\\n# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint\\nmax.logtail.concurrent.requests=5\\n\\n# Max number of concurrent WebSocket calls allowed\\nmax.webSockets.concurrent.requests=15\\n\\n# Pipeline Sharing / ACLs\\npipeline.access.control.enabled=false\\n\\n# Customize header title for TRANSFORMER UI\\n# You can pass any HTML tags here\\n# Example:\\n#   For Text  -  <span class=\\\"navbar-brand\\\">New Brand Name</span>\\n#   For Image -  <img src=\\\"assets/add.png\\\">\\nui.header.title=<span class=\\\"navbar-brand\\\">Transformer</span>\\n\\nui.local.help.base.url=/docs\\nui.hosted.help.base.url=https://www.streamsets.com/documentation/transformer/3.7.0-SNAPSHOT/userguide/help\\n# ui.account.registration.url=\\n\\nui.refresh.interval.ms=2000\\nui.jvmMetrics.refresh.interval.ms=4000\\n\\n# If true TRANSFORMER UI will use WebSocket to fetch pipeline status/metrics/alerts otherwise UI will poll every few seconds\\n# to get the Pipeline status/metrics/alerts.\\nui.enable.webSocket=true\\n\\n# Number of changes supported by undo/redo functionality.\\n# UI archives Pipeline Configuration/Rules in browser memory to support undo/redo functionality.\\nui.undo.limit=10\\n\\n# SMTP configuration to send alert emails\\n# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'\\nmail.transport.protocol=smtp\\nmail.smtp.host=localhost\\nmail.smtp.port=25\\nmail.smtp.auth=false\\nmail.smtp.starttls.enable=false\\nmail.smtps.host=localhost\\nmail.smtps.port=465\\nmail.smtps.auth=false\\n# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,\\n# ${file(\\\"email-password.txt\\\")} will load the value from the 'email-password.txt' file in the config directory (where this file is)\\nxmail.username=foo\\nxmail.password=${file(\\\"email-password.txt\\\")}\\n# FROM email address to use for the messages\\nxmail.from.address=transformer@localhost\\n\\n#Indicates the location where runtime configuration properties can be found.\\n#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with\\n#'runtime.conf_'.\\n#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration\\n#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.\\nruntime.conf.location=embedded\\n\\n# Java Security properties\\n#\\n# Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security\\n# as part of TRANSFORMER bootstrap process. This will change JVM configuration and should not be used when embedding and running\\n# multiple TRANSFORMER instances inside the same JVM.\\n#\\n# We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.\\njava.security.networkaddress.cache.ttl=0\\n\\n# Stage specific configuration(s)\\n#\\n# The following config properties are for particular stages, please refer to their documentation for further details.\\n#\\n# Hadoop components\\n# Uncomment to enforce Hadoop components in TRANSFORMER to always impersonate current user rather then use the impersonation\\n# configuration option. Current user is a user who either started the pipeline or run preview.\\n#hadoop.always.impersonate.current.user=true\\n# Uncomment to enforce impersonated user name to be lower cased.\\n#hadoop.always.lowercase.user=true\\n\\n# Indicate when Transformer runs on MapR cluster.\\n#hadoop.mapr.cluster=false\\n\\n#Observer related\\n\\n#The size of the queueName where the pipeline queues up data rule evaluation requests.\\n#Each request is for a stream and contains sampled records for all rules that apply to that lane.\\nobserver.queue.size=100\\n\\n#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is\\n#once cache per data rule\\nobserver.sampled.records.cache.size=100\\n\\n#The time to wait before dropping a data rule evaluation request if the observer queueName is full.\\nobserver.queue.offer.max.wait.time.ms=1000\\n\\n\\n#Maximum number of private classloaders to allow in Transformer.\\n#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders\\nmax.stage.private.classloaders=50\\n\\n# Pipeline com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner pool\\n# Default value is sufficient to run 22 pipelines. One pipeline requires 5 Threads and pipelines share\\n# threads using thread pool. Approximate com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner thread pool size = (Number of Running Pipelines) * 2.2.\\n# Increasing this value will not increase parallelisation of individual pipelines.\\ncom.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.thread.pool.size=50\\n\\n# Uncomment to disable starting all previously running pipelines on TRANSFORMER start up\\n#com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.boot.pipeline.restart=false\\n\\n# Support bundles\\n#\\n# Uncomment if you need to disable the facility for automatic support bundle upload.\\n#bundle.upload.enabled=false\\n#\\n# Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle\\n# can be time consuming task (depending on size and internet speed) and pipelines can appear \\\"frozen\\\" during\\n# the upload especially when many pipelines are failing at the same time.\\n#bundle.upload.on_error=true\\n\\n#\\n# Additional Configuration files to include in to the configuration.\\n# Value of this property is the name of the configuration file separated by commas.\\n#\\nconfig.includes=vault.properties,credential-stores.properties\\n\\n#\\n# Pipeline State are cached for faster access.\\n# Specifies the maximum number of pipeline state entries the cache may contain.\\nstore.pipeline.state.cache.maximum.size=100\\n\\n# Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration\\n# has elapsed after the entry's creation, the most recent replacement of its value, or its last access.\\n# In minutes\\nstore.pipeline.state.cache.expire.after.access=10\\n\\n# uncomment to skip the Spark version >= 2.3.0 check performed in the Spark app launcher\\n#skipSparkVersionCheck=true\\n\\n# base directory for temporary keytabs (used when keytabs are resolved from credential stores)\\n#transformer.temp-keytabs.location=/tmp/streamsets-transformer\\n\\n#\\n# Copyright 2019 StreamSets Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n\\n#\\n# Control Hub Enabled\\n# If true, HTTP Authentication for Transformer will be configured to use Control Hub SSO Authentication.\\n#\\ndpm.enabled=false\\n\\n#\\n# Base URL of the Remote Service\\n# In a real deployment the security service must be encrypted (HTTPS)\\n#\\ndpm.base.url=http://localhost:18631\\n\\n#\\n# Registration attempts.\\n# There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.\\n#\\ndpm.registration.retry.attempts=5\\n\\n#\\n# Frequency of validation of user and app authentication tokens.\\n# As part of this validation all information about the principal is refreshed.\\n#\\ndpm.security.validationTokenFrequency.secs=60\\n\\n#\\n# Application Token\\n#\\ndpm.appAuthToken=@application-token.txt@\\n\\n#\\n# Labels for this Transformer to report the Control Hub\\n#\\ndpm.remote.control.job.labels=all\\n\\n#\\n# Transformer Ping Frequency to Control Hub (in milliseconds)\\n#\\ndpm.remote.control.ping.frequency=5000\\n\\n#\\n# App to send remote control events\\n#\\ndpm.remote.control.events.recipient=jobrunner-app\\n\\n#\\n# Apps to send Transformer process metrics (CPU Load and Heap Memory Usage)\\n#\\ndpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app\\n\\n#\\n# Frequency to send pipeline status events (all remote pipelines and local running pipelines) and\\n# Transformer process metrics like CPU load and heap memory usage\\n#\\ndpm.remote.control.status.events.interval = 60000\\n\\n\\ndpm.remote.deployment.id=\\n\\n#\\n# Indicates if the redirection to Control Hub SSO is done using HTML META refresh.\\n# This is useful for environment that rewrite redirect headers.\\n#\\nhttp.meta.redirect.to.sso=false\\n\\n\\n# Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting\\n# only takes effect when Transformer is control hub enabled and stage impersonation is set to true\\n#\\n#dpm.alias.name.enabled=true\\n\\n#\\n# Component Type\\n#\\ndpm.componentType=transformer\"
