"[ {\n  \"fileName\" : \"transformer.properties\",\n  \"fileContent\" : 
\"#\\n# Copyright (c) 2021 StreamSets Inc.\\n#\\n\\n# HTTP configuration\\n\\n# The base URL of Transformer, used to create email alert messages.\\n# If not set http://<hostname>:<http.port> is used.\\n# <hostname> is either taken from http.bindHost or resolved using\\n# 'hostname -f' if not configured.\\n#transformer.base.http.url=http://<hostname>:<port>\\n\\n# Hostname or IP address that Transformer will bind to.\\n# Default is 0.0.0.0 that will bind to all interfaces.\\n#http.bindHost=0.0.0.0\\n\\n# Maximum number of HTTP servicing threads.\\n#http.maxThreads=200\\n\\n# The port Transformer runs as the Transformer HTTP endpoint.\\n# If different that -1, Transformer will run on this port.\\n# If 0, Transformer will pick up a random port.\\n# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint\\n# will redirect to the HTTPS endpoint.\\nhttp.port=19630\\n\\n# HTTPS configuration\\n\\n# The port Transformer runs as the Transformer HTTPS endpoint.\\n# If different that -1, Transformer will run over SSL on this port.\\n# If 0, the Transformer will pick up a random port.\\nhttps.port=-1\\n\\n# Enables HTTP/2 support for the Transformer UI/REST API. If you are using any clients\\n# that do not support ALPN for protocol negotiation, leave this option disabled.\\nhttp2.enable=false\\n\\n# Reverse Proxy / Load Balancer configuration\\n\\n# TRANSFORMER will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port\\n# headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.\\n# Set to true when hosting TRANSFORMER behind a reverse proxy / load balancer.\\nhttp.enable.forwarded.requests=false\\n\\n# Java keystore file, in the TRANSFORMER 'etc/' configuration directory\\nhttps.keystore.path=keystore.jks\\n\\n# Password for the keystore file,\\n# By default, the password is loaded from the 'keystore-password.txt'\\n# from the TRANSFORMER 'etc/' configuration directory\\nhttps.keystore.password=${file(\\\"keystore-password.txt\\\")}\\n\\n# Truststore configs\\n# By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore\\n\\n# Java truststore file on Spark driver transformer which stores certificates to trust identity of transformer launcher\\n# in the TRANSFORMER 'etc/' configuration directory.\\n#https.truststore.path=truststore.jks\\n\\n# Password for truststore file\\n# from the TRANSFORMER 'etc/' configuration directory\\n#https.truststore.password=${file(\\\"truststore-password.txt\\\")}\\n\\n# HTTP Session Timeout\\n# Max period of inactivity, after which the HTTP session is invalidated, in seconds.\\n# Default value is 86400 seconds (24 hours)\\n# value -1 means no timeout\\nhttp.session.max.inactive.interval=86400\\n\\n# The authentication for the HTTP endpoint of Transformer.\\n# Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'\\n#\\nhttp.authentication=form\\n\\n# Authentication Login Module\\n# Valid values are: 'file' and 'ldap'\\n# For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,\\n#   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).\\n# For 'ldap', the authentication and role information is read from a LDAP Server\\n#   and LDAP connection information is read from etc/ldap-login.conf.\\nhttp.authentication.login.module=file\\n\\n# The realm used for authentication\\n# A file with the realm name and '.properties' extension must exist in the Transformer configuration directory\\n# If this property is not set, the realm name is '<http.authentication>-realm'\\n#http.digest.realm=local-realm\\n\\n# Check the permissions of the realm file should be owner only\\nhttp.realm.file.permission.check=true\\n\\n# LDAP group to Transformer role mapping\\n# the mapping is specified as the following pattern:\\n#    <ldap-group>:<transformer-role>(,<transformer-role>)*(;<ldap-group>:<transformer-role>(,<transformer-role>)*)*\\n# e.g.\\n#    Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;\\nhttp.authentication.ldap.role.mapping=\\n\\n# LDAP login module name as present in the JAAS config file.\\n# If no value is specified, the login module name is assumed to be \\\"ldap\\\"\\nldap.login.module.name=ldap\\n\\n# HTTP access control (CORS)\\nhttp.access.control.allow.origin=*\\nhttp.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call\\nhttp.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET\\nhttp.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD\\n\\n# Application callback inactivity timeout in milliseconds\\n# Max period of Transformer Spark application callback request inactivity, before stopping the pipeline with a run error.\\n# The default value is 30000 milliseconds (30 seconds)\\ntransformer.driver.max.inactive.interval=30000\\n\\n# A maximum number of times to retry an unsuccessful pipeline start on Databricks Cluster.\\ntransformer.databricks.run.max.retries=2\\n\\n# Minimal interval in milliseconds between the start of the failed run, and the subsequent retry run on the Databricks cluster.\\ntransformer.databricks.run.retry.interval=10000\\n\\n# Always resolve Kerberos properties\\n# This option allows Transformer pipelines to reference the keytab and principal declared in this properties file\\n# If you have a valid kerberos.client.principal and kerberos.client.keytab set below, and want to make them\\n# available to pipelines within this Transformer instance, then comment out the following line\\n#kerberos.client.alwaysResolveProperties=true\\n\\n# The kerberos principal to use for the Kerberos session.\\n# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',\\n# the hostname will be replaced with the actual complete hostname of Transformer as advertised by the\\n# unix command 'hostname -f'\\nkerberos.client.principal=transformer/_HOST@EXAMPLE.COM\\n\\n# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be\\n# looked under the Transformer configuration directory\\nkerberos.client.keytab=transformer.keytab\\n\\n# Uncomment this line to disallow users to specify a keytab to use with pipelines.\\n# If this option is set to false, then the --proxy-user option will be set to spark-submit and the\\n# Transformer process will assume that a ticket cache exists already (i.e. is managed by a wrapper script\\n# such as k5start) and is therefore available for the spark-submit child process\\n#kerberos.pipeline.keytab.allowed=false\\n\\npreview.maxBatchSize=1000\\npreview.maxBatches=10\\n\\n# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint\\nmax.logtail.concurrent.requests=5\\n\\n# Max number of concurrent WebSocket calls allowed\\nmax.webSockets.concurrent.requests=15\\n\\n# Pipeline Sharing / ACLs\\npipeline.access.control.enabled=false\\n\\n# Customize header title for TRANSFORMER UI\\n# You can pass any HTML tags here\\n# Example:\\n#   For Text  -  <span class=\\\"navbar-brand\\\">New Brand Name</span>\\n#   For Image -  <img src=\\\"assets/add.png\\\">\\nui.header.title=<span class=\\\"navbar-brand\\\">Transformer</span>\\n\\nui.local.help.base.url=/docs\\nui.hosted.help.base.url=https://www.streamsets.com/documentation/transformer/3.7.0-SNAPSHOT/userguide/help\\n# ui.account.registration.url=\\n\\nui.refresh.interval.ms=2000\\nui.jvmMetrics.refresh.interval.ms=4000\\n\\n# If true TRANSFORMER UI will use WebSocket to fetch pipeline status/metrics/alerts otherwise UI will poll every few seconds\\n# to get the Pipeline status/metrics/alerts.\\nui.enable.webSocket=true\\n\\n# Number of changes supported by undo/redo functionality.\\n# UI archives Pipeline Configuration/Rules in browser memory to support undo/redo functionality.\\nui.undo.limit=10\\n\\n# SMTP configuration to send alert emails\\n# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'\\nmail.transport.protocol=smtp\\nmail.smtp.host=localhost\\nmail.smtp.port=25\\nmail.smtp.auth=false\\nmail.smtp.starttls.enable=false\\nmail.smtps.host=localhost\\nmail.smtps.port=465\\nmail.smtps.auth=false\\n# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,\\n# ${file(\\\"email-password.txt\\\")} will load the value from the 'email-password.txt' file in the config directory (where this file is)\\nxmail.username=foo\\nxmail.password=${file(\\\"email-password.txt\\\")}\\n# FROM email address to use for the messages\\nxmail.from.address=transformer@localhost\\n\\n#Indicates the location where runtime configuration properties can be found.\\n#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with\\n#'runtime.conf_'.\\n#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration\\n#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.\\nruntime.conf.location=embedded\\n\\n# Java Security properties\\n#\\n# Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security\\n# as part of TRANSFORMER bootstrap process. This will change JVM configuration and should not be used when embedding and running\\n# multiple TRANSFORMER instances inside the same JVM.\\n#\\n# We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.\\njava.security.networkaddress.cache.ttl=0\\n\\n# Stage specific configuration(s)\\n#\\n# The following config properties are for particular stages, please refer to their documentation for further details.\\n#\\n# Hadoop components\\n# Uncomment to enforce Hadoop components in TRANSFORMER to always impersonate current user rather then use the impersonation\\n# configuration option. Current user is a user who either started the pipeline or run preview.\\n#hadoop.always.impersonate.current.user=true\\n# Uncomment to enforce impersonated user name to be lower cased.\\n#hadoop.always.lowercase.user=true\\n\\n# Indicate when Transformer runs on MapR cluster.\\n#hadoop.mapr.cluster=false\\n\\n#Observer related\\n\\n#The size of the queueName where the pipeline queues up data rule evaluation requests.\\n#Each request is for a stream and contains sampled records for all rules that apply to that lane.\\nobserver.queue.size=100\\n\\n#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is\\n#once cache per data rule\\nobserver.sampled.records.cache.size=100\\n\\n#The time to wait before dropping a data rule evaluation request if the observer queueName is full.\\nobserver.queue.offer.max.wait.time.ms=1000\\n\\n\\n#Maximum number of private classloaders to allow in Transformer.\\n#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders\\nmax.stage.private.classloaders=50\\n\\n# Pipeline com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner pool\\n# Default value is sufficient to run 22 pipelines. One pipeline requires 5 Threads and pipelines share\\n# threads using thread pool. Approximate com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner thread pool size = (Number of Running Pipelines) * 2.2.\\n# Increasing this value will not increase parallelisation of individual pipelines.\\ncom.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.thread.pool.size=50\\n\\n# Uncomment to disable starting all previously running pipelines on TRANSFORMER start up\\n#com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.boot.pipeline.restart=false\\n\\n# Support bundles\\n#\\n# Uncomment if you need to disable the facility for automatic support bundle upload.\\n#bundle.upload.enabled=false\\n#\\n# Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle\\n# can be time consuming task (depending on size and internet speed) and pipelines can appear \\\"frozen\\\" during\\n# the upload especially when many pipelines are failing at the same time.\\n#bundle.upload.on_error=true\\n\\n#\\n# Additional Configuration files to include in to the configuration.\\n# Value of this property is the name of the configuration file separated by commas.\\n#\\nconfig.includes=vault.properties,credential-stores.properties\\n\\n#\\n# Pipeline State are cached for faster access.\\n# Specifies the maximum number of pipeline state entries the cache may contain.\\nstore.pipeline.state.cache.maximum.size=100\\n\\n# Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration\\n# has elapsed after the entry's creation, the most recent replacement of its value, or its last access.\\n# In minutes\\nstore.pipeline.state.cache.expire.after.access=10\\n\\n# uncomment to skip the Spark version >= 2.3.0 check performed in the Spark app launcher\\n#skipSparkVersionCheck=true\\n\\n# base directory for temporary keytabs (used when keytabs are resolved from credential stores)\\n#transformer.temp-keytabs.location=/tmp/streamsets-transformer\\n\\n#\\n# Copyright 2019 StreamSets Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n\\n#\\n# Control Hub Enabled\\n# If true, HTTP Authentication for Transformer will be configured to use Control Hub SSO Authentication.\\n#\\ndpm.enabled=false\\n\\n#\\n# Base URL of the Remote Service\\n# In a real deployment the security service must be encrypted (HTTPS)\\n#\\ndpm.base.url=http://localhost:18631\\n\\n#\\n# Registration attempts.\\n# There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.\\n#\\ndpm.registration.retry.attempts=5\\n\\n#\\n# Frequency of validation of user and app authentication tokens.\\n# As part of this validation all information about the principal is refreshed.\\n#\\ndpm.security.validationTokenFrequency.secs=60\\n\\n#\\n# Application Token\\n#\\ndpm.appAuthToken=@application-token.txt@\\n\\n#\\n# Labels for this Transformer to report the Control Hub\\n#\\ndpm.remote.control.job.labels=all\\n\\n#\\n# Transformer Ping Frequency to Control Hub (in milliseconds)\\n#\\ndpm.remote.control.ping.frequency=5000\\n\\n#\\n# App to send remote control events\\n#\\ndpm.remote.control.events.recipient=jobrunner-app\\n\\n#\\n# Apps to send Transformer process metrics (CPU Load and Heap Memory Usage)\\n#\\ndpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app\\n\\n#\\n# Frequency to send pipeline status events (all remote pipelines and local running pipelines) and\\n# Transformer process metrics like CPU load and heap memory usage\\n#\\ndpm.remote.control.status.events.interval = 60000\\n\\n\\ndpm.remote.deployment.id=\\n\\n#\\n# Indicates if the redirection to Control Hub SSO is done using HTML META refresh.\\n# This is useful for environment that rewrite redirect headers.\\n#\\nhttp.meta.redirect.to.sso=false\\n\\n\\n# Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting\\n# only takes effect when Transformer is control hub enabled and stage impersonation is set to true\\n#\\n#dpm.alias.name.enabled=true\\n\\n#\\n# Component Type\\n#\\ndpm.componentType=transformer\"
\n},
{\n  \"fileName\" : \"credential-stores.properties\",\n  \"fileContent\" : 
\"#\\n# Copyright (c) 2021 StreamSets Inc.\\n#\\n\\n# Use this file to enable the use of credential stores with Transformer.\\n\\n# IMPORTANT: This file includes a set of properties for each credential store type.\\n# Property names include the default credential store IDs: jks, azure, aws.\\n# When you use custom IDs, you must update the corresponding property names.\\n\\n# IMPORTANT: This file includes a set of properties for each credential store type.\\n# Property names include the default credential store IDs: jks, azure, aws.\\n# When you use custom IDs, you must update the corresponding property names.\\n\\n# To use multiple credential stores of the same type, make sure each credential store\\n# has a set of related properties defined. The property names must be updated with\\n# the appropriate credential store ID.\\n\\n################################################\\n#        Transformer Credential Stores         #\\n################################################\\n\\n# Defines the credential stores for Transformer to use. Specify a comma-separated list\\n# of unique credential store IDs.\\n#credentialStores=jks,azure,aws,vault\\n\\n################################################\\n# Java Keystore Credential Store Configuration #\\n################################################\\n\\n# The following properties are for a Java keystore credential store that uses the 'jks'\\n# default credential store ID. If you specified a custom ID in the credentialStores property\\n# above, replace 'jks' in the property names with the custom ID.\\n\\n# Defines the implementation of the 'jks' credential store\\n# Update 'jks' in the property name as needed, but do not change the definition of this property.\\ncredentialStore.jks.def=streamsets-transformer-jks-credentialstore-lib::com_streamsets_datacollector_credential_javakeystore_JavaKeyStoreCredentialStore\\n\\n# A Java Keystore credential store can be of type JCEKS or PKCS12\\ncredentialStore.jks.config.keystore.type=PKCS12\\n\\n# The location of the Java keystore. Specify an absolute path or a path relative to the\\n# $TRANSFORMER_CONF directory.\\ncredentialStore.jks.config.keystore.file=jks-credentialStore.pkcs12\\n\\n# The password to access the Java keystore\\ncredentialStore.jks.config.keystore.storePassword=changeIt\\n\\n########################################################\\n#    Azure Key Vault Credential Store Configuration    #\\n########################################################\\n\\n# The following properties are for an Azure Key Vault credential store that uses the 'azure'\\n# default credential store ID. If you specified a custom ID in the credentialStores property,\\n# replace 'azure' in the property names with the custom ID.\\n\\n# Defines the implementation of the 'azure' credential store\\n# Update 'azure' in the property name as needed, but do not change the definition of this property.\\ncredentialStore.azure.def=streamsets-transformer-azure-keyvault-credentialstore-lib::com_streamsets_datacollector_credential_azure_keyvault_AzureKeyVaultCredentialStore\\n\\n# Credential refresh interval\\n# How long a credential can be cached locally before fetching it again from Azure Key Vault.\\ncredentialStore.azure.config.credential.refresh.millis=30000\\n\\n# Credential retry interval\\n# How long to wait before retrying to fetch a credential from Azure Key Vault in case of errors.\\n# This retry delay is not blocking. Locally, it will fail immediately.\\ncredentialStore.azure.config.credential.retry.millis=15000\\n\\n# Azure Key Vault credential provider URL\\n# This property must be set.\\n# credentialStore.azure.config.vault.url=https://<YOUR_KEY_VAULT>.vault.azure.net/\\n\\n# Azure Key Vault client ID for this Transformer\\n#credentialStore.azure.config.client.id=<MUST BE SET>\\n\\n# Azure Key Vault client key for this Transformer\\n#credentialStore.azure.config.client.key=<MUST BE SET>\\n\\n# Requires a group secret for each secret\\ncredentialStore.azure.config.enforceEntryGroup=false\\n\\n############################################################\\n#    AWS Secrets Manager Credential Store Configuration    #\\n############################################################\\n\\n# The following properties are for an AWS Secrets Manager credential store that uses the 'aws'\\n# default credential store ID. If you specified a custom ID in the credentialStores property,\\n# replace 'aws' in the property names with the custom ID.\\n\\n# Defines the implementation of the 'aws' credential store\\n# Update 'aws' in the property name as needed, but do not change the definition of this property.\\ncredentialStore.aws.def=streamsets-transformer-aws-secrets-manager-credentialstore-lib::com_streamsets_datacollector_credential_aws_secrets_manager_AWSSecretsManagerCredentialStore\\n\\n# Default name-key separator for the name parameter in credential functions\\ncredentialStore.aws.config.nameKey.separator=&\\n\\n# AWS region\\ncredentialStore.aws.config.region=<MUST BE SET>\\n\\n# AWS access key\\ncredentialStore.aws.config.access.key=<MUST BE SET>\\n\\n# AWS secret key\\ncredentialStore.aws.config.secret.key=<MUST BE SET>\\n\\n# Secrets max cache size\\n# Maximum number of secrets to cache locally\\ncredentialStore.aws.config.cache.max.size=1024\\n\\n# Secrets cache TTL\\n# The number of milliseconds that a cached secret is considered valid before requiring a refresh\\n# The default is equivalent to 1 hour\\ncredentialStore.aws.config.cache.ttl.millis=3600000\\n\\n# Requires a group secret for each secret\\ncredentialStore.aws.config.enforceEntryGroup=false\\n\\n########################################################\\n#    Hashicorp Vault Credential Store Configuration    #\\n########################################################\\n\\n# The following properties are for a Hashicorp Vault credential store that uses the 'vault'\\n# default credential store ID. If you specified a custom ID in the credentialStores property,\\n# replace 'vault' in the property names with the custom ID.\\n\\n# Defines the implementation of the 'vault' credential store\\n# Update 'vault' in the property name as needed, but do not change the definition of this property.\\ncredentialStore.vault.def=streamsets-transformer-vault-credentialstore-lib::com_streamsets_datacollector_credential_vault_VaultCredentialStore\\n\\n# Default path-key separator for the name parameter in credential functions\\ncredentialStore.vault.config.pathKey.separator=&\\n\\n# URL of the Vault server to connect to\\ncredentialStore.vault.config.addr=http://localhost:8200\\n\\n# AppRole mode (recommended)\\ncredentialStore.vault.config.role.id=\\ncredentialStore.vault.config.secret.id=\\n\\n#\\n# The Vault User ID is generated by hashing the MAC address belonging to the network interface assigned\\n# the IP address of hostname -f. It can also be retrieved by the show-vault-id command of the\\n# StreamSets executable.\\n#\\n\\n# Transformer authenticates with Vault using the AppId authentication backend. The app-id must be specified below.\\n# credentialStore.vault.config.app.id=\\n\\n# Optional Settings\\n\\n# Supported KV Secret Engine version 1 by default. Possible values: 1 or 2.\\ncredentialStore.vault.config.version=1\\n\\n# Define namespaces for Vault Enterprise\\n#credentialStore.vault.config.namespace=\\n\\n# The renewal interval must be shorter than the shortest lease issued by Vault including auth tokens.\\ncredentialStore.vault.config.lease.renewal.interval.sec=60\\ncredentialStore.vault.config.lease.expiration.buffer.sec=120\\ncredentialStore.vault.config.open.timeout=0\\ncredentialStore.vault.config.proxy.address=\\ncredentialStore.vault.config.proxy.port=8080\\ncredentialStore.vault.config.proxy.username=\\ncredentialStore.vault.config.proxy.password=\\ncredentialStore.vault.config.read.timeout=0\\ncredentialStore.vault.config.ssl.enabled.protocols=TLSv1.2,TLSv1.3\\ncredentialStore.vault.config.ssl.truststore.file=\\ncredentialStore.vault.config.ssl.truststore.password=\\ncredentialStore.vault.config.ssl.verify=true\\ncredentialStore.vault.config.ssl.timeout=0\\ncredentialStore.vault.config.timeout=0\\n\\n# Requires a group secret for each secret\\ncredentialStore.vault.config.enforceEntryGroup=false\"
\n}, 
{\n  \"fileName\" : \"transformer-log4j.properties\",\n  \"fileContent\" : 
\"#\\n# Copyright 2021 StreamSets Inc.\\n#\\n\\n# /dev/null appender\\nlog4j.appender.null=org.apache.log4j.FileAppender\\nlog4j.appender.null.File=/dev/null\\nlog4j.appender.null.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.null.layout.ConversionPattern=null\\n\\n# <stdout> appender\\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\\nlog4j.appender.stdout.Target=System.out\\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} [user:%X{s-user}] [pipeline:%X{s-entity}] [com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner:%X{s-com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner}] [thread:%t] [stage:%X{s-stage}] %-5p %c{1} - %m%n\\n\\n# sdc.log appender\\nlog4j.appender.streamsets=org.apache.log4j.RollingFileAppender\\nlog4j.appender.streamsets.MaxFileSize=256MB\\nlog4j.appender.streamsets.MaxBackupIndex=10\\nlog4j.appender.streamsets.File=${transformer.log.dir}/transformer.log\\nlog4j.appender.streamsets.Append=true\\nlog4j.appender.streamsets.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.streamsets.layout.ConversionPattern=%d{ISO8601} [user:%X{s-user}] [pipeline:%X{s-entity}] [com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner:%X{s-com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner}] [thread:%t] [stage:%X{s-stage}] %-5p %c{1} - %m%n\\n\\n# spark-submit launcher log appender\\nlog4j.appender.sparkSubmitLauncher=org.apache.log4j.RollingFileAppender\\nlog4j.appender.sparkSubmitLauncher.MaxFileSize=256MB\\nlog4j.appender.sparkSubmitLauncher.MaxBackupIndex=5\\nlog4j.appender.sparkSubmitLauncher.File=${transformer.log.dir}/spark-submit-launcher.log\\nlog4j.appender.sparkSubmitLauncher.Append=true\\nlog4j.appender.sparkSubmitLauncher.layout=org.apache.log4j.EnhancedPatternLayout\\nlog4j.appender.sparkSubmitLauncher.layout.ConversionPattern=%c{1} %m%throwable%n\\n\\nlog4j.rootLogger=INFO, streamsets\\nlog4j.logger.com.streamsets=INFO\\nlog4j.logger.org.eclipse.jetty=WARN\\nlog4j.logger.com.amazonaws.services.kinesis.clientlibrary.lib.worker.SequenceNumberValidator=WARN\\nlog4j.logger.org.apache.kafka=WARN\\n\\n# log messages from the spark-submit process to the spark-submit launcher appender\\nlog4j.logger.org.apache.spark.launcher.app=DEBUG, sparkSubmitLauncher\\n# also log the special BEGIN and END marker messages to the same appender\\n# keep in-sync with the logger name defined in SparkSubmitAppLauncher\\nlog4j.logger.TransformerSparkSubmitLauncherMarker=INFO, sparkSubmitLauncher\\n# but we don't need those marker messages to appear in the main transformer.log appender\\nlog4j.additivity.TransformerSparkSubmitLauncherMarker=false\\n\"
\n},
{\n  \"fileName\" : \"transformer-security.policy\",\n  \"fileContent\" : 
\"// Copyright 2021 StreamSets Inc.\\n//\\n\\n// StreamSets Transformer Policy File\\n\\n// StreamSets code base:\\ngrant codebase \\\"file://${transformer.dist.dir}/libexec/bootstrap-libs/-\\\" {\\n  permission java.security.AllPermission;\\n};\\ngrant codebase \\\"file://${transformer.dist.dir}/root-lib/*\\\" {\\n  permission java.security.AllPermission;\\n};\\ngrant codebase \\\"file://${transformer.dist.dir}/api-lib/*\\\" {\\n  permission java.security.AllPermission;\\n};\\ngrant codebase \\\"file://${sdc.asterClientLib.dir}/*\\\" {\\n  permission java.security.AllPermission;\\n};\\ngrant codebase \\\"file://${transformer.dist.dir}/container-lib/*\\\" {\\n  permission java.security.AllPermission;\\n};\\ngrant codebase \\\"file://${transformer.dist.dir}/externalResources/streamsets-libs-extras/-\\\" {\\n  permission java.security.AllPermission;\\n};\\n// StreamSets stage libraries code base:\\ngrant codebase \\\"file://${transformer.dist.dir}/streamsets-libs/-\\\" {\\n  permission java.security.AllPermission;\\n};\\n\\n// Groovy will parse files in a different context, so we need to grant it additional privileges\\ngrant codebase \\\"file:/groovy/script\\\" {\\n  permission java.lang.RuntimePermission \\\"getClassLoader\\\";\\n};\\n\\n// For details on how to grant specific permissions, refer to the Java Permissions Documentation:\\n//   http://docs.oracle.com/javase/7/docs/technotes/guides/security/permissions.html\\n\\n// User stage libraries code base:\\ngrant codebase \\\"file://${transformer.dist.dir}/externalResources/user-libs/-\\\" {\\n  permission java.util.PropertyPermission \\\"*\\\", \\\"read\\\";\\n  permission java.lang.RuntimePermission \\\"accessDeclaredMembers\\\";\\n  permission java.lang.reflect.ReflectPermission \\\"suppressAccessChecks\\\";\\n};\\n\\n// For JARs to be available to all stage libraries\\ngrant codebase \\\"file://${transformer.dist.dir}/libs-common-lib/-\\\" {\\n  permission java.security.AllPermission;\\n};\\n\"
\n} ]"
